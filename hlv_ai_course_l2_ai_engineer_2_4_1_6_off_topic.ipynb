{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWj-CFeTJ9xb"
      },
      "source": [
        "**‚ö†Ô∏è IMPORTANT! ‚ö†Ô∏è**\n",
        "\n",
        "---\n",
        "\n",
        "**üîì The Notebook is read-only:** Use **File ‚ñ∏ Save a copy in Drive** to make it yours, then run cells ‚úÖ\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìö **This Notebook** is part of the **Input Validation & Guardrails** series:\n",
        "\n",
        "- [Input Validation](https://colab.research.google.com/drive/1bQwv3o1dBfTlTz4koGF5AXFt3ArnwwtA#copy=true) - Basic input validation\n",
        "- [Toxicity Detection](https://colab.research.google.com/drive/1cd73fGf-DkyM_tMHrh_eYQ7NWWsn5wgd#copy=true) ‚Äî Block harmful content\n",
        "- [PII Detection](https://colab.research.google.com/drive/1hcNw8n8w3oEk2MfGQoG1XuJphfbhcgwM#copy=true) ‚Äî Protect sensitive information\n",
        "- [Prompt Injection Detection](https://colab.research.google.com/drive/1eNuATWd2HbuLUvJptiXgmsAgWkgwPVDv#copy=true) ‚Äî Prevent manipulation attempts\n",
        "- [Off-Topic Detection](https://colab.research.google.com/drive/1cd73fGf-DkyM_tMHrh_eYQ7NWWsn5wgd#copy=true) ‚Äî Keep conversations focused"
      ],
      "metadata": {
        "id": "vSu7VxtknwEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üéØ Input Validation & Guardrails\n",
        "\n",
        "## Off-Topic Detection for LLM Applications\n",
        "\n",
        "Welcome to this notebook on **Off-Topic Detection** ‚Äî the final line of defense that ensures your RAG system stays focused on its intended purpose!\n",
        "\n",
        "Even if a query is safe, non-toxic, and well-formed, it might simply be **irrelevant** to your system's domain. If your RAG system is built to answer questions about \"Company HR Policy,\" it has no business answering questions about \"Python coding\" or \"History of Rome.\"\n"
      ],
      "metadata": {
        "id": "fo6WaDbcK3wY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üö´ Why Off-Topic Detection Matters\n",
        "\n",
        "Without this guardrail, your system will attempt to retrieve documents for irrelevant queries. Since vector search **always returns something** (even if the match is poor), the LLM will receive random, irrelevant context chunks.\n",
        "\n",
        "This leads to three major problems:\n",
        "\n",
        "| Problem | Description | Impact |\n",
        "|---------|-------------|--------|\n",
        "| **ü§ñ Hallucinations** | LLM tries to force an answer using irrelevant context | Incorrect/misleading responses |\n",
        "| **üí∏ Resource Waste** | You pay for embedding, retrieval, and generation on unanswerable queries | Wasted costs |\n",
        "| **üè∑Ô∏è Brand Risk** | Your specialized bot acts like a general-purpose chatbot | Dilutes product purpose |\n",
        "\n",
        "### Example Scenario:\n",
        "```text\n",
        "User asks: \"What is the capital of France?\" (to your HR Policy bot)\n",
        "\n",
        "‚ùå Without Off-Topic Detection:\n",
        "   ‚Üí Vector search returns random HR docs with low similarity\n",
        "   ‚Üí LLM receives irrelevant context about vacation policies\n",
        "   ‚Üí LLM either hallucinates or gives a confused answer\n",
        "   ‚Üí You paid for the full pipeline for a question you can't answer\n",
        "\n",
        "‚úÖ With Off-Topic Detection:\n",
        "   ‚Üí Query flagged as off-topic (low similarity to HR anchors)\n",
        "   ‚Üí Immediate response: \"I can only help with HR-related questions.\"\n",
        "   ‚Üí No wasted resources, clear user guidance\n",
        "```"
      ],
      "metadata": {
        "id": "zsAjcDQwLDSj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WczsIX4fGd4I"
      },
      "source": [
        "## üìã Prerequisites\n",
        "\n",
        "Before we start, make sure you have:\n",
        "- ‚úÖ A Google Colab environment (you're already here!)\n",
        "- ‚úÖ Internet connection\n",
        "- ‚úÖ An OpenAI API Key (we'll guide you through this)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvOTJr44GfUC"
      },
      "source": [
        "## üîß Environment Setup\n",
        "\n",
        "First, let's install the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVflwnsIGpxd"
      },
      "outputs": [],
      "source": [
        "# Install the required libraries\n",
        "!pip install openai sentence-transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwEtKfQZGtyj"
      },
      "outputs": [],
      "source": [
        "# Import all required libraries\n",
        "from openai import OpenAI\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3r07f_bG7tt"
      },
      "source": [
        "## üîë Secure API Key Management\n",
        "\n",
        "### Obtaining Your API Key\n",
        "\n",
        "To get your API key:\n",
        "\n",
        "1. Go to [Hooloovoo Help Desk - AI Control Center](https://hooloovoo.atlassian.net/servicedesk/customer/portal/48)\n",
        "2. Navigate to **AI API Keys** section\n",
        "3. Submit the request (NOTE: Provider should be OpenAI)\n",
        "\n",
        "### Security Best Practices\n",
        "\n",
        "**DO:**\n",
        "- ‚úÖ Store keys as environment variables or in secure storage\n",
        "- ‚úÖ Use separate keys for development and production\n",
        "- ‚úÖ Set usage limits and monitoring\n",
        "- ‚úÖ Rotate keys periodically\n",
        "\n",
        "**DON'T:**\n",
        "- ‚ùå Hard-code keys in your source code\n",
        "- ‚ùå Commit keys to version control\n",
        "- ‚ùå Share keys publicly or in screenshots\n",
        "- ‚ùå Share your keys with others\n",
        "\n",
        "### Setting Up Your API Key\n",
        "\n",
        "Run the cell below and paste your API key when prompted (it will be hidden for security)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LekjKz6HDjr"
      },
      "outputs": [],
      "source": [
        "# Securely input your API key\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Prompt for API key (input will be hidden)\n",
        "api_key = getpass(\"Enter your OpenAI API Key: \")\n",
        "\n",
        "# Set it as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "print(\"‚úÖ API key configured successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Helper Function\n",
        "\n",
        "Let's create a simple helper function to make API calls easier to read and understand."
      ],
      "metadata": {
        "id": "72wpesB5LY4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def call_llm(prompt, model=\"gpt-4.1-nano\", temperature=0.7):\n",
        "\n",
        "    # Build parameters dictionary\n",
        "    params = {\n",
        "        \"model\": model,\n",
        "        \"input\": prompt,\n",
        "    }\n",
        "\n",
        "    if not model == \"gpt-5-codex\":\n",
        "      params[\"temperature\"] = temperature\n",
        "\n",
        "    response = client.responses.create(**params)\n",
        "    return response.output_text\n",
        "\n",
        "print(\"‚úÖ Helper function defined!\")"
      ],
      "metadata": {
        "id": "husu2bnaLY4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß≠ The Approach: Semantic Boundary Checking\n",
        "\n",
        "Instead of training a classifier to recognize every possible off-topic category (which is impossible), we use **Similarity-Based Validation**:\n",
        "\n",
        "1. **Define anchor queries** ‚Äî A list of \"canonical\" on-topic example queries\n",
        "2. **Embed the anchors** ‚Äî Convert them to vectors in semantic space\n",
        "3. **Compare incoming queries** ‚Äî Check cosine similarity against anchors\n",
        "4. **Apply threshold** ‚Äî If similarity is too low, reject as off-topic\n"
      ],
      "metadata": {
        "id": "6jmHEvLnLbUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è Building the Topic Guardrail\n",
        "\n",
        "Let's create a `TopicGuardrail` class that:\n",
        "1. Takes a list of on-topic example queries as \"anchors\"\n",
        "2. Embeds them into vector space\n",
        "3. Compares new queries against these anchors using cosine similarity\n",
        "4. Rejects queries that fall below the similarity threshold"
      ],
      "metadata": {
        "id": "LLyijR2fLvnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import Optional\n",
        "\n",
        "class TopicGuardrail:\n",
        "    \"\"\"\n",
        "    Semantic boundary checker for off-topic detection.\n",
        "    Uses cosine similarity against anchor queries to determine relevance.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        on_topic_examples: list[str],\n",
        "        threshold: float = 0.35,\n",
        "        model_name: str = 'all-MiniLM-L6-v2'\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the topic guardrail.\n",
        "\n",
        "        Args:\n",
        "            on_topic_examples: List of example queries that ARE in scope\n",
        "            threshold: Minimum similarity to be considered on-topic (0.0 to 1.0)\n",
        "            model_name: Sentence transformer model to use\n",
        "        \"\"\"\n",
        "        print(f\"Loading embedding model: {model_name}...\")\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.threshold = threshold\n",
        "        self.on_topic_examples = on_topic_examples\n",
        "\n",
        "        # Pre-compute embeddings for all on-topic examples\n",
        "        print(f\"Embedding {len(on_topic_examples)} anchor queries...\")\n",
        "        self.topic_embeddings = self.model.encode(on_topic_examples)\n",
        "\n",
        "        print(\"‚úÖ Topic guardrail ready!\")\n",
        "\n",
        "    def check(self, query: str) -> tuple[bool, dict]:\n",
        "        \"\"\"\n",
        "        Check if a query is on-topic.\n",
        "\n",
        "        Args:\n",
        "            query: The user's input query\n",
        "\n",
        "        Returns:\n",
        "            tuple: (is_on_topic, details)\n",
        "                - is_on_topic: True if query is within scope\n",
        "                - details: Dict with similarity scores and matched anchor\n",
        "        \"\"\"\n",
        "        # Embed the incoming query\n",
        "        query_embedding = self.model.encode([query])[0]\n",
        "\n",
        "        # Calculate cosine similarity against all anchor embeddings\n",
        "        # Cosine similarity = dot(A, B) / (norm(A) * norm(B))\n",
        "        similarities = np.dot(self.topic_embeddings, query_embedding) / (\n",
        "            np.linalg.norm(self.topic_embeddings, axis=1) *\n",
        "            np.linalg.norm(query_embedding)\n",
        "        )\n",
        "\n",
        "        # Find the best matching anchor\n",
        "        max_idx = int(np.argmax(similarities))\n",
        "        max_similarity = float(similarities[max_idx])\n",
        "        best_match = self.on_topic_examples[max_idx]\n",
        "\n",
        "        # Determine if on-topic based on threshold\n",
        "        is_on_topic = max_similarity >= self.threshold\n",
        "\n",
        "        return is_on_topic, {\n",
        "            \"max_similarity\": max_similarity,\n",
        "            \"best_match\": best_match,\n",
        "            \"threshold\": self.threshold,\n",
        "            \"all_similarities\": dict(zip(self.on_topic_examples, similarities.tolist()))\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ TopicGuardrail class defined!\")"
      ],
      "metadata": {
        "id": "0vbNUrAHL382"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üè¢ Example: HR Policy Assistant\n",
        "\n",
        "Let's create a topic guardrail for an **HR Policy Assistant** that should only answer questions about company policies, benefits, and HR procedures."
      ],
      "metadata": {
        "id": "H14ghixfMNrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define anchor queries for an HR Policy Assistant\n",
        "hr_anchor_queries = [\n",
        "    # Vacation & Time Off\n",
        "    \"What is our vacation policy?\",\n",
        "    \"How do I request time off?\",\n",
        "    \"How many sick days do I get?\",\n",
        "\n",
        "    # Benefits\n",
        "    \"What are the health insurance options?\",\n",
        "    \"How does the 401k matching work?\",\n",
        "    \"What is the parental leave policy?\",\n",
        "\n",
        "    # Procedures\n",
        "    \"How do I submit an expense report?\",\n",
        "    \"What is the dress code?\",\n",
        "    \"How do I report harassment?\",\n",
        "\n",
        "    # General HR\n",
        "    \"What are the office hours?\",\n",
        "    \"How do I update my direct deposit?\",\n",
        "    \"What is the remote work policy?\",\n",
        "]\n",
        "\n",
        "# Create the topic guardrail\n",
        "hr_guardrail = TopicGuardrail(\n",
        "    on_topic_examples=hr_anchor_queries,\n",
        "    threshold=0.35  # Queries must have at least 35% similarity to be on-topic\n",
        ")"
      ],
      "metadata": {
        "id": "XNVLLi4GMRSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Testing On-Topic Queries\n",
        "\n",
        "Let's test with queries that SHOULD be allowed ‚Äî they're related to HR topics."
      ],
      "metadata": {
        "id": "1C_6NEn0Msp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with on-topic queries\n",
        "on_topic_queries = [\n",
        "    \"How many vacation days do I get per year?\",\n",
        "    \"Can I work from home on Fridays?\",\n",
        "    \"What's the process for requesting maternity leave?\",\n",
        "    \"Does the company match 401k contributions?\",\n",
        "    \"Where do I submit my travel expenses?\",\n",
        "]\n",
        "\n",
        "print(\"Testing ON-TOPIC queries:\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for query in on_topic_queries:\n",
        "    is_on_topic, details = hr_guardrail.check(query)\n",
        "    status = \"‚úÖ ON-TOPIC\" if is_on_topic else \"‚ùå OFF-TOPIC\"\n",
        "\n",
        "    print(f\"\\nQuery: \\\"{query}\\\"\")\n",
        "    print(f\"Result: {status}\")\n",
        "    print(f\"Similarity: {details['max_similarity']:.2f} (threshold: {details['threshold']})\")\n",
        "    print(f\"Best match: \\\"{details['best_match']}\\\"\")"
      ],
      "metadata": {
        "id": "0ayKMWRwMv2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ùå Testing Off-Topic Queries\n",
        "\n",
        "Now let's test with queries that should be REJECTED ‚Äî they have nothing to do with HR."
      ],
      "metadata": {
        "id": "bVdlkziTMzPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with off-topic queries\n",
        "off_topic_queries = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"How do I write a Python function?\",\n",
        "    \"What's the weather like today?\",\n",
        "    \"Can you help me with my math homework?\",\n",
        "    \"Who won the Super Bowl last year?\",\n",
        "    \"What is machine learning?\",\n",
        "]\n",
        "\n",
        "print(\"Testing OFF-TOPIC queries:\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for query in off_topic_queries:\n",
        "    is_on_topic, details = hr_guardrail.check(query)\n",
        "    status = \"‚úÖ ON-TOPIC\" if is_on_topic else \"‚ùå OFF-TOPIC\"\n",
        "\n",
        "    print(f\"\\nQuery: \\\"{query}\\\"\")\n",
        "    print(f\"Result: {status}\")\n",
        "    print(f\"Similarity: {details['max_similarity']:.2f} (threshold: {details['threshold']})\")\n",
        "    print(f\"Best match: \\\"{details['best_match']}\\\"\")"
      ],
      "metadata": {
        "id": "TTMagGyHM3hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Visualizing the Similarity Scores\n",
        "\n",
        "Let's create a visual comparison showing how on-topic vs off-topic queries score against our anchors."
      ],
      "metadata": {
        "id": "rV28J7-yM63u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined visualization\n",
        "all_test_queries = [\n",
        "    (\"On-topic\", \"How many vacation days do I get?\"),\n",
        "    (\"On-topic\", \"What's the health insurance deductible?\"),\n",
        "    (\"On-topic\", \"Can I expense my home office setup?\"),\n",
        "    (\"Off-topic\", \"What is the capital of France?\"),\n",
        "    (\"Off-topic\", \"How do I cook pasta?\"),\n",
        "    (\"Off-topic\", \"Explain quantum computing\"),\n",
        "]\n",
        "\n",
        "print(\"üìä SIMILARITY SCORE VISUALIZATION\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Threshold: {hr_guardrail.threshold}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for category, query in all_test_queries:\n",
        "    is_on_topic, details = hr_guardrail.check(query)\n",
        "    score = details[\"max_similarity\"]\n",
        "\n",
        "    # Create visual bar\n",
        "    bar_length = int(score * 40)\n",
        "    threshold_pos = int(hr_guardrail.threshold * 40)\n",
        "\n",
        "    # Build the bar with threshold marker\n",
        "    bar = \"\"\n",
        "    for i in range(40):\n",
        "        if i == threshold_pos:\n",
        "            bar += \"|\"  # Threshold marker\n",
        "        elif i < bar_length:\n",
        "            bar += \"‚ñà\"\n",
        "        else:\n",
        "            bar += \"‚ñë\"\n",
        "\n",
        "    status = \"‚úÖ\" if is_on_topic else \"‚ùå\"\n",
        "\n",
        "    print(f\"\\n[{category}] \\\"{query[:35]}{'...' if len(query) > 35 else ''}\\\"\")\n",
        "    print(f\"  [{bar}] {score:.2f} {status}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Legend: | = threshold, ‚ñà = similarity score\")"
      ],
      "metadata": {
        "id": "o3Z506rdM-BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Tuning the Threshold\n",
        "\n",
        "The threshold is crucial for balancing **coverage** vs **precision**:\n",
        "\n",
        "| Threshold | Effect | Use Case |\n",
        "|-----------|--------|----------|\n",
        "| **Lower (0.25)** | More permissive, allows borderline queries | General-purpose assistants |\n",
        "| **Medium (0.35)** | Balanced approach | Most production systems |\n",
        "| **Higher (0.50)** | More strict, only very relevant queries pass | High-precision systems |\n",
        "\n",
        "Let's see how different thresholds affect our results:"
      ],
      "metadata": {
        "id": "ymzaTqUNNG-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test different thresholds\n",
        "test_queries = [\n",
        "    (\"How many PTO days do new employees get?\", \"Clear HR question\"),\n",
        "    (\"What's the company policy on side projects?\", \"Borderline HR question\"),\n",
        "    (\"How do I improve my productivity at work?\", \"General work question\"),\n",
        "    (\"What programming languages should I learn?\", \"Off-topic tech question\"),\n",
        "]\n",
        "\n",
        "thresholds = [0.25, 0.35, 0.45, 0.55]\n",
        "\n",
        "print(\"üìà THRESHOLD SENSITIVITY ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Header\n",
        "print(f\"\\n{'Query':<40} | \", end=\"\")\n",
        "for t in thresholds:\n",
        "    print(f\"t={t:<5} | \", end=\"\")\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "\n",
        "for query, description in test_queries:\n",
        "    # Get the base similarity score\n",
        "    _, details = hr_guardrail.check(query)\n",
        "    score = details[\"max_similarity\"]\n",
        "\n",
        "    # Display query\n",
        "    display_query = query[:38] + \"..\" if len(query) > 40 else query\n",
        "    print(f\"{display_query:<40} | \", end=\"\")\n",
        "\n",
        "    # Check against each threshold\n",
        "    for t in thresholds:\n",
        "        status = \"‚úÖ\" if score >= t else \"‚ùå\"\n",
        "        print(f\"{status} {score:.2f} | \", end=\"\")\n",
        "\n",
        "    print(f\" ({description})\")"
      ],
      "metadata": {
        "id": "gn66c0zmNGn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üéØ Improving Embedding Accuracy: Fine-Tuning Considerations\n",
        "\n",
        "While the `all-MiniLM-L6-v2` model works well for general use cases, you may find that it doesn't capture the nuances of your specific domain. Here are strategies to improve accuracy:\n",
        "\n",
        "### When Embedding-Based Detection Falls Short\n",
        "\n",
        "| Symptom | Cause | Solution |\n",
        "|---------|-------|----------|\n",
        "| Too many false positives | Threshold too high, or model doesn't understand domain | Lower threshold OR fine-tune model |\n",
        "| Too many false negatives | Threshold too low, or anchors don't cover all topics | Add more anchors OR fine-tune model |\n",
        "| Borderline queries misclassified | General-purpose embeddings lack domain specificity | Fine-tune on domain data |\n",
        "\n",
        "### Fine-Tuning Open-Source Embedding Models\n",
        "\n",
        "If you have labeled data (queries marked as on-topic or off-topic), you can fine-tune open-source models for better accuracy:\n",
        "\n",
        "**Popular Models for Fine-Tuning:**\n",
        "- [`sentence-transformers/all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) ‚Äî Fast, good baseline\n",
        "- [`sentence-transformers/all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) ‚Äî Higher quality, 5x slower\n",
        "- [`BAAI/bge-small-en-v1.5`](https://huggingface.co/BAAI/bge-small-en-v1.5) ‚Äî Excellent for retrieval tasks\n",
        "- [`intfloat/e5-small-v2`](https://huggingface.co/intfloat/e5-small-v2) ‚Äî Strong performance on semantic similarity\n",
        "\n",
        "**Fine-Tuning Approach (Conceptual):**\n",
        "\n",
        "```python\n",
        "# Conceptual example - not executable in this notebook\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1. Prepare training data as pairs\n",
        "train_examples = [\n",
        "    # (query, anchor, label) - label=1 if similar, 0 if not\n",
        "    InputExample(texts=[\"How do I get time off?\", \"What is our vacation policy?\"], label=1.0),\n",
        "    InputExample(texts=[\"What's the weather?\", \"What is our vacation policy?\"], label=0.0),\n",
        "    # ... more examples\n",
        "]\n",
        "\n",
        "# 2. Create dataloader\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
        "\n",
        "# 3. Define loss function\n",
        "train_loss = losses.CosineSimilarityLoss(model)\n",
        "\n",
        "# 4. Fine-tune\n",
        "model.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    epochs=3,\n",
        "    warmup_steps=100\n",
        ")\n",
        "```\n",
        "\n",
        "**When to Fine-Tune:**\n",
        "- You have 500+ labeled query pairs\n",
        "- Domain-specific vocabulary (legal, medical, technical)\n",
        "- Current accuracy is below 85% on your test set\n",
        "- False positive/negative rate is unacceptable for your use case\n",
        "\n",
        "> *üí° **Tip:***\n",
        ">\n",
        "> *Start with more anchors and threshold tuning before investing in fine-tuning. Often, adding 10-20 well-chosen anchor queries can dramatically improve accuracy without the complexity of training.*"
      ],
      "metadata": {
        "id": "MdEJxUhEO-5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# ü§ñ A Different Approach: LLM-Based Off-Topic Detection\n",
        "\n",
        "While embedding-based detection is fast and cost-effective, **LLM-based classification** offers superior accuracy for nuanced decisions. Modern small LLMs like `gpt-4.1-nano` can perform sophisticated reasoning about topic relevance at minimal cost.\n",
        "\n",
        "### Why Use LLM for Off-Topic Detection?\n",
        "\n",
        "| Advantage | Description |\n",
        "|-----------|-------------|\n",
        "| **üéØ Higher Accuracy** | LLMs understand context, intent, and nuance better than cosine similarity |\n",
        "| **üß† Reasoning Capability** | Can explain WHY a query is off-topic, useful for debugging |\n",
        "| **üìù No Anchor Management** | Describe your domain in natural language instead of curating examples |\n",
        "| **üîÑ Easy Updates** | Change the system description without re-embedding anchors |\n",
        "| **üåê Handles Edge Cases** | Better at borderline queries that embeddings might misclassify |\n",
        "\n",
        "### When to Choose LLM Over Embeddings?\n",
        "\n",
        "| Use Case | Best Approach |\n",
        "|----------|---------------|\n",
        "| High volume, low latency (>1000 req/min) | Embeddings |\n",
        "| High accuracy requirements | LLM |\n",
        "| Limited anchor examples available | LLM |\n",
        "| Complex, nuanced domain boundaries | LLM |\n",
        "| Cost-sensitive production | Embeddings |\n",
        "| Borderline query handling | LLM |"
      ],
      "metadata": {
        "id": "BCtxlhSocNc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "\n",
        "class LLMTopicGuardrail:\n",
        "    \"\"\"\n",
        "    LLM-based off-topic detection using fast, efficient classification.\n",
        "    Uses a small model (gpt-4.1-nano) for cost-effective, accurate decisions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, domain_description: str, allowed_topics: list[str]):\n",
        "        \"\"\"\n",
        "        Initialize the LLM topic guardrail.\n",
        "\n",
        "        Args:\n",
        "            domain_description: Clear description of what the system handles\n",
        "            allowed_topics: List of allowed topic categories\n",
        "        \"\"\"\n",
        "        self.domain_description = domain_description\n",
        "        self.allowed_topics = allowed_topics\n",
        "        self.model = \"gpt-4.1-nano\"  # Fast, cheap, accurate for classification\n",
        "\n",
        "        # Build the classification prompt template\n",
        "        self.system_prompt = self._build_system_prompt()\n",
        "\n",
        "    def _build_system_prompt(self) -> str:\n",
        "        \"\"\"Build an optimized system prompt for fast classification.\"\"\"\n",
        "        topics_str = \", \".join(self.allowed_topics)\n",
        "\n",
        "        return f\"\"\"You are a query classifier. Your task is to determine if a user query is ON-TOPIC or OFF-TOPIC.\n",
        "\n",
        "DOMAIN: {self.domain_description}\n",
        "\n",
        "ALLOWED TOPICS: {topics_str}\n",
        "\n",
        "RULES:\n",
        "- ON-TOPIC: Query relates to any of the allowed topics\n",
        "- OFF-TOPIC: Query is unrelated to the domain (general knowledge, other domains, chitchat)\n",
        "\n",
        "Respond with ONLY valid JSON in this exact format:\n",
        "{{\"decision\": \"ON-TOPIC\" or \"OFF-TOPIC\", \"reason\": \"brief explanation\"}}\"\"\"\n",
        "\n",
        "    def check(self, query: str) -> tuple[bool, dict]:\n",
        "        \"\"\"\n",
        "        Check if a query is on-topic using LLM classification.\n",
        "\n",
        "        Args:\n",
        "            query: The user's input query\n",
        "\n",
        "        Returns:\n",
        "            tuple: (is_on_topic, details)\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Build the prompt\n",
        "        prompt = f\"{self.system_prompt}\\n\\nQUERY: {query}\\n\\nJSON:\"\n",
        "\n",
        "        try:\n",
        "            # Call LLM with low temperature for consistent classification\n",
        "            response = call_llm(prompt, model=self.model, temperature=0)\n",
        "\n",
        "            # Parse JSON response\n",
        "            result = json.loads(response.strip())\n",
        "\n",
        "            is_on_topic = result.get(\"decision\", \"\").upper() == \"ON-TOPIC\"\n",
        "            reason = result.get(\"reason\", \"No reason provided\")\n",
        "\n",
        "        except (json.JSONDecodeError, Exception) as e:\n",
        "            # Fallback: try to extract decision from raw text\n",
        "            print(f\"‚ö†Ô∏è Error parsing JSON: {e}\")\n",
        "            response_upper = response.upper() if 'response' in dir() else \"\"\n",
        "            is_on_topic = \"ON-TOPIC\" in response_upper and \"OFF-TOPIC\" not in response_upper\n",
        "            reason = f\"Parse error, inferred from response\"\n",
        "\n",
        "        elapsed_ms = (time.time() - start_time) * 1000\n",
        "\n",
        "        return is_on_topic, {\n",
        "            \"decision\": \"ON-TOPIC\" if is_on_topic else \"OFF-TOPIC\",\n",
        "            \"reason\": reason,\n",
        "            \"latency_ms\": elapsed_ms,\n",
        "            \"model\": self.model\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ LLMTopicGuardrail class defined!\")"
      ],
      "metadata": {
        "id": "XcXRu7oGccHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üè¢ Example: HR Policy Assistant with LLM Detection\n",
        "\n",
        "Let's create an LLM-based topic guardrail for the same HR Policy Assistant scenario."
      ],
      "metadata": {
        "id": "Igbx2qt-cqgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an LLM-based topic guardrail for HR\n",
        "llm_hr_guardrail = LLMTopicGuardrail(\n",
        "    domain_description=\"Company HR Policy Assistant that answers questions about employee policies, benefits, procedures, and workplace guidelines.\",\n",
        "    allowed_topics=[\n",
        "        \"vacation and time off\",\n",
        "        \"health insurance and benefits\",\n",
        "        \"401k and retirement\",\n",
        "        \"parental leave\",\n",
        "        \"expense reports\",\n",
        "        \"dress code\",\n",
        "        \"harassment reporting\",\n",
        "        \"remote work policies\",\n",
        "        \"office hours\",\n",
        "        \"payroll and compensation\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"‚úÖ LLM HR Topic Guardrail created!\")\n",
        "print(f\"   Model: {llm_hr_guardrail.model}\")\n",
        "print(f\"   Topics: {len(llm_hr_guardrail.allowed_topics)} categories\")"
      ],
      "metadata": {
        "id": "6Thakg27c4B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Testing LLM-Based Detection\n",
        "\n",
        "Let's test the LLM guardrail with a variety of queries and see how it performs."
      ],
      "metadata": {
        "id": "PCLlKLljcwqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test queries - mix of on-topic and off-topic\n",
        "test_queries = [\n",
        "    # Clear on-topic\n",
        "    (\"How many vacation days do new employees get?\", \"On-topic\"),\n",
        "    (\"What's the process for requesting parental leave?\", \"On-topic\"),\n",
        "    (\"Does the company match 401k contributions?\", \"On-topic\"),\n",
        "\n",
        "    # Borderline / nuanced\n",
        "    (\"Can I work from a coffee shop?\", \"Borderline - remote work related\"),\n",
        "    (\"What should I wear to a client meeting?\", \"Borderline - dress code related\"),\n",
        "    (\"How do I improve my work-life balance?\", \"Borderline - could be HR or general\"),\n",
        "\n",
        "    # Clear off-topic\n",
        "    (\"What is the capital of France?\", \"Off-topic\"),\n",
        "    (\"How do I write a Python function?\", \"Off-topic\"),\n",
        "    (\"What's the best pizza topping?\", \"Off-topic\"),\n",
        "]\n",
        "\n",
        "print(\"ü§ñ LLM-BASED OFF-TOPIC DETECTION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "total_latency = 0\n",
        "for query, expected in test_queries:\n",
        "    is_on_topic, details = llm_hr_guardrail.check(query)\n",
        "    total_latency += details[\"latency_ms\"]\n",
        "\n",
        "    status = \"‚úÖ ON-TOPIC\" if is_on_topic else \"‚ùå OFF-TOPIC\"\n",
        "\n",
        "    print(f\"\\nüìù Query: \\\"{query}\\\"\")\n",
        "    print(f\"   Expected: {expected}\")\n",
        "    print(f\"   Result: {status}\")\n",
        "    print(f\"   Reason: {details['reason']}\")\n",
        "    print(f\"   Latency: {details['latency_ms']:.0f}ms\")\n",
        "\n",
        "avg_latency = total_latency / len(test_queries)\n",
        "print(f\"\\n\" + \"=\" * 70)\n",
        "print(f\"üìä Average latency: {avg_latency:.0f}ms per query\")"
      ],
      "metadata": {
        "id": "yd9yNMfecpEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ö°Ô∏è Latency optimizations\n",
        "\n",
        "This optimized guardrail focuses on **reducing tokens, avoiding retries, and enabling cache hits**:\n",
        "\n",
        "- **System prompt kept separate & stable** (sent as a system message, not concatenated into a big string). This reduces prompt churn and is better for caching.\n",
        "- **Structured Outputs (schema-enforced)** so the model *must* return valid JSON ‚Üí no `json.loads()` failures, no fallback logic, no hidden retries.\n",
        "- **Optimize Response** so that model returns `bool` not `string` for `is_on_topic` field. Remove `reason` field (this can be \"enabled\" during testing).\n",
        "- **Hard cap on output length** via `max_output_tokens` (we only need `decision` + optional short `reason`).\n",
        "- **Disable storage** (`store=False`) for slightly leaner requests (and better privacy defaults)."
      ],
      "metadata": {
        "id": "812vdZm-94um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from typing import Literal, Optional, Tuple, Dict, Any, List\n",
        "\n",
        "from openai import OpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class TopicDecision(BaseModel):\n",
        "    is_on_topic: bool\n",
        "    # Make reason optional so you can run \"ultra-fast mode\" with fewer output tokens.\n",
        "    reason: Optional[str] = Field(default=None, max_length=120)\n",
        "\n",
        "\n",
        "class OptimizedLLMTopicGuardrail:\n",
        "    \"\"\"\n",
        "    Optimized LLM-based off-topic detection for low latency.\n",
        "\n",
        "    Key knobs:\n",
        "      - system prompt as a stable system message (better caching, less prompt noise)\n",
        "      - structured outputs (no JSON parsing drama)\n",
        "      - max_output_tokens kept tiny\n",
        "      - prompt caching enabled (prompt_cache_key + prompt_cache_retention)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        domain_description: str,\n",
        "        allowed_topics: List[str],\n",
        "        *,\n",
        "        client: Optional[OpenAI] = None,\n",
        "        model: str = \"gpt-4.1-nano\",\n",
        "        temperature: float = 0.0,\n",
        "        max_output_tokens: int = 40,\n",
        "        include_reason: bool = True,\n",
        "        store: bool = False,\n",
        "    ):\n",
        "        self.client = client or OpenAI()\n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "        self.max_output_tokens = max_output_tokens\n",
        "        self.include_reason = include_reason\n",
        "        self.store = store\n",
        "\n",
        "        # Precompute a compact, stable system prompt once (important for cache hit rate).\n",
        "        topics = \"\\n\".join(f\"- {t.strip()}\" for t in allowed_topics if t and t.strip())\n",
        "        self.system_prompt = (\n",
        "            \"You are a strict topic classifier.\\n\"\n",
        "            \"Return ONLY the JSON object matching the schema.\\n\"\n",
        "            \"Decide on_topic is true if the query clearly relates to the domain/topics; otherwise false.\\n\"\n",
        "            f\"DOMAIN: {domain_description.strip()}\\n\"\n",
        "            f\"TOPICS:\\n{topics}\\n\"\n",
        "        )\n",
        "\n",
        "    def check(self, query: str, *, cache_key: Optional[str] = None) -> Tuple[bool, Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            query: user input\n",
        "            cache_key: stable identifier to improve cache hit rates (e.g., hashed user/org id)\n",
        "\n",
        "        Returns:\n",
        "            (is_on_topic, details_dict)\n",
        "        \"\"\"\n",
        "        start = time.time()\n",
        "\n",
        "        # Keep the user message tiny. The heavy, stable prefix is in the system message.\n",
        "        input_messages = [\n",
        "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    f\"QUERY: {query}\\n\"\n",
        "                    + (\"Include a short reason.\" if self.include_reason else \"No reason.\")\n",
        "                ),\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            # Structured outputs: no JSON parsing, no retry loops for formatting.\n",
        "            # max_output_tokens: hard cap on generated tokens.\n",
        "            resp = self.client.responses.parse(\n",
        "                model=self.model,\n",
        "                input=input_messages,\n",
        "                text_format=TopicDecision,\n",
        "                temperature=self.temperature,\n",
        "                max_output_tokens=self.max_output_tokens,\n",
        "                store=self.store\n",
        "            )\n",
        "\n",
        "            parsed: TopicDecision = resp.output_parsed\n",
        "\n",
        "            latency_ms = (time.time() - start) * 1000\n",
        "            usage = getattr(resp, \"usage\", None)\n",
        "\n",
        "            return parsed.is_on_topic, {\n",
        "                \"reason\": parsed.reason or \"\",\n",
        "                \"latency_ms\": latency_ms,\n",
        "                \"model\": self.model,\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            # Fail-safe behavior for a guardrail: treat as OFF_TOPIC (or route to stricter path).\n",
        "            latency_ms = (time.time() - start) * 1000\n",
        "            return False, {\n",
        "                \"reason\": f\"classification_error: {type(e).__name__}\",\n",
        "                \"latency_ms\": latency_ms,\n",
        "                \"model\": self.model,\n",
        "            }"
      ],
      "metadata": {
        "id": "TXrJQWAH_Qx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do another test, first create new guardrail object:"
      ],
      "metadata": {
        "id": "dwNOckjl_Y7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_llm_hr_guardrail = OptimizedLLMTopicGuardrail(\n",
        "    domain_description=\"Company HR Policy Assistant that answers questions about employee policies, benefits, procedures, and workplace guidelines.\",\n",
        "    allowed_topics=[\n",
        "        \"vacation and time off\",\n",
        "        \"health insurance and benefits\",\n",
        "        \"401k and retirement\",\n",
        "        \"parental leave\",\n",
        "        \"expense reports\",\n",
        "        \"dress code\",\n",
        "        \"harassment reporting\",\n",
        "        \"remote work policies\",\n",
        "        \"office hours\",\n",
        "        \"payroll and compensation\"\n",
        "    ],\n",
        "    client=client,\n",
        "    include_reason=False  # Disable include reason during \"production\". Enable during testing, good while tweaking the prompt, temperature or model choice.\n",
        ")\n",
        "\n",
        "print(\"‚úÖ LLM HR Topic Guardrail created!\")\n",
        "print(f\"   Model: {llm_hr_guardrail.model}\")\n",
        "print(f\"   Topics: {len(llm_hr_guardrail.allowed_topics)} categories\")"
      ],
      "metadata": {
        "id": "8z3jrPnm_nCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test queries - mix of on-topic and off-topic\n",
        "test_queries = [\n",
        "    # Clear on-topic\n",
        "    (\"How many vacation days do new employees get?\", \"On-topic\"),\n",
        "    (\"What's the process for requesting parental leave?\", \"On-topic\"),\n",
        "    (\"Does the company match 401k contributions?\", \"On-topic\"),\n",
        "\n",
        "    # Borderline / nuanced\n",
        "    (\"Can I work from a coffee shop?\", \"Borderline - remote work related\"),\n",
        "    (\"What should I wear to a client meeting?\", \"Borderline - dress code related\"),\n",
        "    (\"How do I improve my work-life balance?\", \"Borderline - could be HR or general\"),\n",
        "\n",
        "    # Clear off-topic\n",
        "    (\"What is the capital of France?\", \"Off-topic\"),\n",
        "    (\"How do I write a Python function?\", \"Off-topic\"),\n",
        "    (\"What's the best pizza topping?\", \"Off-topic\"),\n",
        "]\n",
        "\n",
        "print(\"ü§ñ LLM-BASED OFF-TOPIC DETECTION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "total_latency = 0\n",
        "for query, expected in test_queries:\n",
        "    is_on_topic, details = llm_hr_guardrail.check(query)\n",
        "    total_latency += details[\"latency_ms\"]\n",
        "\n",
        "    status = \"‚úÖ ON-TOPIC\" if is_on_topic else \"‚ùå OFF-TOPIC\"\n",
        "\n",
        "    print(f\"\\nüìù Query: \\\"{query}\\\"\")\n",
        "    print(f\"   Expected: {expected}\")\n",
        "    print(f\"   Result: {status}\")\n",
        "    print(f\"   Reason: {details['reason']}\")\n",
        "    print(f\"   Latency: {details['latency_ms']:.0f}ms\")\n",
        "\n",
        "avg_latency = total_latency / len(test_queries)\n",
        "print(f\"\\n\" + \"=\" * 70)\n",
        "print(f\"üìä Average latency: {avg_latency:.0f}ms per query\")"
      ],
      "metadata": {
        "id": "QvXxsSSK_aiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ö° Performance Comparison: Embeddings vs LLM\n",
        "\n",
        "Let's compare both approaches on the same set of queries to see the trade-offs."
      ],
      "metadata": {
        "id": "-rCLM86TdEZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare both approaches\n",
        "comparison_queries = [\n",
        "    \"How do I request time off for a medical appointment?\",\n",
        "    \"What benefits are available for part-time employees?\",\n",
        "    \"Can I expense my home office furniture?\",\n",
        "    \"What programming language should I learn?\",\n",
        "    \"Who won the World Cup?\",\n",
        "    \"Is there a policy about bringing pets to the office?\",  # Borderline\n",
        "]\n",
        "\n",
        "print(\"‚ö° EMBEDDING vs LLM COMPARISON\")\n",
        "print(\"=\" * 75)\n",
        "print(f\"{'Query':<45} {'Embedding':<15} {'LLM':<15}\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "embedding_times = []\n",
        "llm_times = []\n",
        "\n",
        "for query in comparison_queries:\n",
        "    # Embedding approach\n",
        "    start = time.time()\n",
        "    emb_on_topic, emb_details = hr_guardrail.check(query)\n",
        "    emb_time = (time.time() - start) * 1000\n",
        "    embedding_times.append(emb_time)\n",
        "\n",
        "    # LLM approach\n",
        "    llm_on_topic, llm_details = optimized_llm_hr_guardrail.check(query)\n",
        "    llm_times.append(llm_details[\"latency_ms\"])\n",
        "\n",
        "    # Format results\n",
        "    emb_result = f\"{'‚úÖ' if emb_on_topic else '‚ùå'} ({emb_details['max_similarity']:.2f})\"\n",
        "    llm_result = f\"{'‚úÖ' if llm_on_topic else '‚ùå'}\"\n",
        "\n",
        "    display_query = query[:43] + \"..\" if len(query) > 45 else query\n",
        "    print(f\"{display_query:<45} {emb_result:<15} {llm_result:<15}\")\n",
        "\n",
        "print(\"-\" * 75)\n",
        "print(f\"{'Average Latency:':<45} {sum(embedding_times)/len(embedding_times):.0f}ms{'':<9} {sum(llm_times)/len(llm_times):.0f}ms\")"
      ],
      "metadata": {
        "id": "BsxHuP_qdH1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ Choosing the Right Approach\n",
        "\n",
        "Based on our comparison, here's a decision framework:\n",
        "\n",
        "| Factor | Embedding-Based | LLM-Based |\n",
        "|--------|-----------------|-----------|\n",
        "| **Latency** | ~10-50ms | ~500-1500ms |\n",
        "| **Cost per query** | ~\\$0.00001 (compute only) | ~\\$0.0001-0.001 |\n",
        "| **Accuracy on clear cases** | Excellent | Excellent |\n",
        "| **Accuracy on borderline** | Good | Excellent |\n",
        "| **Explainability** | Limited (similarity score) | High (reasoning provided) |\n",
        "| **Setup effort** | Requires anchor curation | Just describe domain |\n",
        "| **Maintenance** | Update anchors manually | Update description |\n",
        "\n",
        "### Recommended Patterns:\n",
        "\n",
        "**Pattern 1: Embedding-First (Cost Optimized)**\n",
        "```text\n",
        "User Query ‚Üí Embedding Check ‚Üí If borderline ‚Üí LLM Check ‚Üí Decision\n",
        "```\n",
        "\n",
        "**Pattern 2: LLM-Only (Accuracy Optimized)**\n",
        "```text\n",
        "User Query ‚Üí LLM Check ‚Üí Decision\n",
        "```\n",
        "\n",
        "**Pattern 3: Parallel (Latency + Accuracy)**\n",
        "```text\n",
        "User Query ‚Üí [Embedding Check || LLM Check] ‚Üí Combine Results ‚Üí Decision\n",
        "```"
      ],
      "metadata": {
        "id": "yGhRplfcdgj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hybrid_topic_check(\n",
        "    query: str,\n",
        "    embedding_guardrail: TopicGuardrail,\n",
        "    llm_guardrail: LLMTopicGuardrail,\n",
        "    borderline_threshold: tuple = (0.30, 0.50)\n",
        ") -> tuple[bool, dict]:\n",
        "    \"\"\"\n",
        "    Hybrid approach: Use embeddings first, escalate to LLM for borderline cases.\n",
        "\n",
        "    This pattern optimizes for both cost and accuracy:\n",
        "    - Clear on-topic (similarity > 0.50): Accept immediately\n",
        "    - Clear off-topic (similarity < 0.30): Reject immediately\n",
        "    - Borderline (0.30-0.50): Use LLM for final decision\n",
        "\n",
        "    Args:\n",
        "        query: User query to check\n",
        "        embedding_guardrail: Pre-configured embedding guardrail\n",
        "        llm_guardrail: Pre-configured LLM guardrail\n",
        "        borderline_threshold: (low, high) thresholds for borderline zone\n",
        "    \"\"\"\n",
        "    low_thresh, high_thresh = borderline_threshold\n",
        "\n",
        "    # Step 1: Fast embedding check\n",
        "    start = time.time()\n",
        "    emb_on_topic, emb_details = embedding_guardrail.check(query)\n",
        "    emb_time = (time.time() - start) * 1000\n",
        "\n",
        "    similarity = emb_details[\"max_similarity\"]\n",
        "\n",
        "    # Step 2: Decide based on confidence\n",
        "    if similarity >= high_thresh:\n",
        "        # High confidence on-topic - no need for LLM\n",
        "        return True, {\n",
        "            \"method\": \"embedding\",\n",
        "            \"decision\": \"ON-TOPIC\",\n",
        "            \"reason\": f\"High similarity ({similarity:.2f}) to '{emb_details['best_match']}'\",\n",
        "            \"similarity\": similarity,\n",
        "            \"latency_ms\": emb_time,\n",
        "            \"llm_used\": False\n",
        "        }\n",
        "\n",
        "    elif similarity < low_thresh:\n",
        "        # High confidence off-topic - no need for LLM\n",
        "        return False, {\n",
        "            \"method\": \"embedding\",\n",
        "            \"decision\": \"OFF-TOPIC\",\n",
        "            \"reason\": f\"Low similarity ({similarity:.2f}) - not related to domain\",\n",
        "            \"similarity\": similarity,\n",
        "            \"latency_ms\": emb_time,\n",
        "            \"llm_used\": False\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        # Borderline case - escalate to LLM\n",
        "        llm_on_topic, llm_details = llm_guardrail.check(query)\n",
        "        total_time = emb_time + llm_details[\"latency_ms\"]\n",
        "\n",
        "        return llm_on_topic, {\n",
        "            \"method\": \"hybrid (escalated to LLM)\",\n",
        "            \"decision\": llm_details[\"decision\"],\n",
        "            \"reason\": llm_details[\"reason\"],\n",
        "            \"similarity\": similarity,\n",
        "            \"latency_ms\": total_time,\n",
        "            \"llm_used\": True\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Hybrid topic check function defined!\")\n",
        "\n",
        "# Cell Type: Code\n",
        "# Test the hybrid approach\n",
        "hybrid_test_queries = [\n",
        "    \"What is our vacation policy?\",           # Clear on-topic\n",
        "    \"What is the capital of France?\",         # Clear off-topic\n",
        "    \"Can I bring my dog to work sometimes?\",  # Borderline\n",
        "    \"How do I handle stress at work?\",        # Borderline\n",
        "    \"What's the dress code for casual Friday?\", # On-topic\n",
        "]\n",
        "\n",
        "print(\"üîÄ HYBRID APPROACH RESULTS\")\n",
        "print(\"=\" * 75)\n",
        "\n",
        "for query in hybrid_test_queries:\n",
        "    is_on_topic, details = hybrid_topic_check(query, hr_guardrail, llm_hr_guardrail)\n",
        "\n",
        "    status = \"‚úÖ ON-TOPIC\" if is_on_topic else \"‚ùå OFF-TOPIC\"\n",
        "    llm_indicator = \"ü§ñ\" if details[\"llm_used\"] else \"üìä\"\n",
        "\n",
        "    print(f\"\\n{llm_indicator} Query: \\\"{query}\\\"\")\n",
        "    print(f\"   Decision: {status}\")\n",
        "    print(f\"   Method: {details['method']}\")\n",
        "    print(f\"   Reason: {details['reason']}\")\n",
        "    print(f\"   Latency: {details['latency_ms']:.0f}ms\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 75)\n",
        "print(\"Legend: üìä = Embedding only, ü§ñ = LLM escalation\")"
      ],
      "metadata": {
        "id": "iyEUz0NGdsHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéì Interactive Demo: Test Your Own Queries\n",
        "\n",
        "Try modifying the query below to see how the HR guardrail responds!"
      ],
      "metadata": {
        "id": "z7MCmEY6NbfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üéØ TRY IT YOURSELF!\n",
        "# Modify this query and run the cell\n",
        "\n",
        "your_query = \"How do I request a day off for a doctor's appointment?\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"YOUR QUERY ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "is_on_topic, details = hr_guardrail.check(your_query)\n",
        "\n",
        "print(f\"\\nüìù Query: \\\"{your_query}\\\"\")\n",
        "print(f\"\\nüìä Results:\")\n",
        "print(f\"   Status: {'‚úÖ ON-TOPIC' if is_on_topic else '‚ùå OFF-TOPIC'}\")\n",
        "print(f\"   Similarity Score: {details['max_similarity']:.3f}\")\n",
        "print(f\"   Threshold: {details['threshold']}\")\n",
        "print(f\"   Best Matching Anchor: \\\"{details['best_match']}\\\"\")\n",
        "\n",
        "# Show top 3 most similar anchors\n",
        "print(f\"\\nüîç Top 3 Most Similar Anchors:\")\n",
        "sorted_sims = sorted(\n",
        "    details['all_similarities'].items(),\n",
        "    key=lambda x: x[1],\n",
        "    reverse=True\n",
        ")[:3]\n",
        "\n",
        "for i, (anchor, sim) in enumerate(sorted_sims, 1):\n",
        "    print(f\"   {i}. \\\"{anchor}\\\" ‚Üí {sim:.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ],
      "metadata": {
        "id": "7hvj1dxjNj1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìö Summary: Off-Topic Detection Approaches\n",
        "\n",
        "| Approach | Best For | Latency | Cost | Accuracy |\n",
        "|----------|----------|---------|------|----------|\n",
        "| **Embedding-Based** | High volume, cost-sensitive | ~10-50ms | Very Low | Good |\n",
        "| **LLM-Based** | High accuracy needs, complex domains | ~100-500ms | Low | Excellent |\n",
        "| **Hybrid** | Balance of cost and accuracy | ~10-500ms | Optimized | Excellent |\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **Start with embeddings** ‚Äî Fast, cheap, and effective for most cases\n",
        "2. **Add LLM for borderline cases** ‚Äî Gets the best of both worlds\n",
        "3. **Consider fine-tuning** ‚Äî If embedding accuracy isn't sufficient\n",
        "4. **Use gpt-4.1-nano** ‚Äî Fast and cheap for classification tasks\n",
        "5. **Design clear domain descriptions** ‚Äî Better prompts = better LLM accuracy"
      ],
      "metadata": {
        "id": "QSexPEiqeAmm"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOj8KOEFP8BcCG3ev4nN/7H"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}